## π“ κ°μΈ ν•™μµ

- 9κ°• κ°•μ λ° μ‹¤μµ μ½”λ“ λ¦¬λ·°

## π” Further Reading

- GPT-1
- BERT : Pre-training of deep bidirectional transformers for language understanding, NAACLβ€™19
- SQuAD: Stanford Question Answering Dataset
- SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference

## π¤ λ…Όλ¬Έ λ¦¬λ·° λ°ν‘ λ° μ§λ‹µ

- GPT-1 λ°ν‘ (κ°•μ§„μ„ )
- BERT λ°ν‘ (μ΄λ…Έμ•„)
- in GPT, fine tuningμ—μ„ lrκ°’μ„ μ™ μ‘κ² μ·¨ν•λ”μ§€? -> λ…Όμ.
- in BERT, λ¶„λ¥ λ¬Έμ λ¥Ό ν’€ λ• λ§μ¤ν‚Ήμ΄ λ λ°μ΄ν„°λ¥Ό μΈν’‹μΌλ΅ λ°›μΌλ©΄ μ›λ BERTμ ν…μ¤ν¬μΈ LMμ΄λ‘ νΉμ„±μ΄ λ‹¬λΌμ„ ν•™μµμ΄ μ–΄λ µλ‹¤κ³  μ•κ³  μμ—λ”λ°, λ§μ¤ν‚Ή ν•κ²ƒμ΄ λ‹¨μν λ‹¨μ–΄κ°€ unknown μ²λ¦¬λμ„ λ•λ‘ λ‹¤λ¥Έ κ±΄κ°€? -> pre-training λ‹¨ μ…μ¥μ—μ„λ” μΈν’‹μΌλ΅ λ¬΄μ—‡μ΄ λ“¤μ–΄μ¤λ“  κ²°κµ­ μ μ‚¬ν•  κ±° κ°™λ‹¤. κ·Έλ¬λ‚ μΈν’‹μ— λ”°λ¥Έ ν…μ¤ν¬μ λ©μ μ΄ λ‹¤λ¥Έ κ²ƒμ΄λ―€λ΅ κ³Όμ  μμ²΄λ” κµ¬λ¶„λ  κ²ƒμ΄λ‹¤.
- in BERT, κ³Όμ—° NSPκ°€ μ •λ§ ν¨κ³Όκ°€ μμ„κΉ? -> μ¶”κ°€ ν•™μµ

## πΈ λ©ν† λ§

- μ‹κ°„ λ³€κ²½ : 1μ‹
- μ§λ¬Έ : λ‹¤μμ£Όλ„ λ©ν† λ§μ΄ μλ‚μ”?
- positional encoding μ •λ¦¬
- μ•μΌλ΅ λ‚μ¬ λ…Όλ¬Έλ“¤μ μ „μ²΄μ μΈ νλ¦„(κ° λ…Όλ¬Έλ“¤μ μ£Όμ” μ°¨μ΄ λ“±) μ†κ°
- ν•™μµ λ…Όλ¬Έμ„ μ„μ£Όλ΅ κµ¬ν„ν•΄ λ³Όλ§ν• μ½”λ“ μ¶”μ²
- μ μµν• μλ‹¤
