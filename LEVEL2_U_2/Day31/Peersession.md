## 학습한 내용

- Attention is all you need, Group Normalization 간단 리뷰
- Attention is not explanation ⇒ 기계가 학습하여 결과는 어느 정도 일관성 있게 나오지만, 결과를 도출해내는 과정에서 근거가 되는 word가 다르다면 어떻게 설명이 된다고 볼 수 있는지 다루는 내용.(진선님)

## 학습할 내용

- 수요일까지 => transformer관련 논문 및 실습 코드 한 번 더 확인 후 피어세션때 공유.
- 목요일까지: 9강 페이지, BERT, GPT-1 논문 읽어오기 (발표자들은 발표 준비 해오기❣)
- 금요일까지: 10강 페이지, 선택 과제2, 선택 과제3

## 특이사항

- Positional Encoding 멘토님께 설명 부탁드리기
