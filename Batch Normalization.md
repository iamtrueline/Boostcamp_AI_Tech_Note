# Batch Normalization

- 신경망이 파라미터 값을 효과적으로 학습하기 위한 방편 중 활성화 함수 변경, 가중치 초기화, lr조정 등이 있다.
- 위같은 방법 외에 학습하는 과정 자체를 전체적으로 안정화하여 학습 속도를 가속 시킬 수 있는 근본적인 방법으로 배치 정규화(Batch Normalization)이 있다.
- 마찬가지로 목적은 Gradient Vanishing / Gradient Exploding 회피.
- 일반적인 배치 단위 학습 수행시, 연산 후에 배치 단위간 데이터 분포량에 편차가 생길 수 있다. 즉, 배치 간 데이터가 상이(Internal Covariant Shift).
- 배치 간 데이터가 상이한 게 뭐가 문제? 입력 분포 형태가 널을 뛰면 안정적인 학습이 어려움.
- 각 배치별 평균과 분산을 이용해 이를 정규화시킨다면 평균 0, 표준편차 1로 데이터 분포를 조정.
- 배치 정규화는 학습 단계와 추론 단계 각 적용이 다름.
- 학습시엔 평균과 분산 계산을 각 배치별로 수행, 추론시엔 고정값을 사용. 이는 추론시엔 배치 단위별로 측정이 어렵기 때문.
