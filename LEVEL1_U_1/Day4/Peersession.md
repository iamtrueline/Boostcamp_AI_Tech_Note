## 🔍 이전 질문 한번더 우창님의 리뷰

- Relu가 왜 비선형 함수인가?
- Relu와 sigmoid 함수의 차이
- KL-Divergence 한번더 공부
- entropy
- cross-entropy -> 예측과 달라서 생기는 깜놀도(정보량)
- https://angeloyeo.github.io/2020/10/27/KL_divergence.html


## 📒 금일 질문 목록

- RNN에서 시퀀스가 너무 길면 Vanishing, Exploding -> BPTT
  - Answer : 시퀀스가 너무 길때, 끝에서나 몇개의 기준을 정해서 전체에 대해 Back Prop 하지않고 정해진 기준에 대해 잘라서 거기 까지만 Back Prop 한다.
- 퀴즈 질문 -> 이 식이 어떻게 성립하는가?
  - 이는 베이시안 법칙에 따라 전개하면 풀린다.
- ariable gradient 의 개념적 이해 -> 변수 축에 따른 gradient라고 생각하면 된다.                      
- 선택과제 1번, Mini Batch에서 복원추출과 비복원추출
- 선택과제 2번 RNN에 대한 토의 -> grad를 어떻게 할것인가?
  - 미분을 구할때 근사하는것이 맞는것인가, 아니면 식을 세워서 하는것이 맞는것인가?
